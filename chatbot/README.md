# ğŸ¤– AI Chatbot

A Flask-based AI chatbot that combines a custom-trained neural network (**NLP intent classifier**) with local **Ollama** LLM support, live weather data, and a geek-jokes API â€” all in one sleek web interface.

---

## âœ¨ Features

| Feature | Details |
|---|---|
| ğŸ§  **NLP Intent Classifier** | TensorFlow/Keras model trained on `intents.json` for fast canned responses |
| ğŸ¦™ **Ollama LLM Integration** | Multi-turn conversations with `llama3.2` & `gemma3:4b`; streaming (SSE) supported |
| ğŸŒ¤ï¸ **Live Weather** | Powered by [Open-Meteo](https://open-meteo.com) â€” no API key needed |
| ğŸ˜„ **Geek Jokes** | Random jokes from the [Geek-Jokes API](https://geek-jokes.sameerkumar.website) â€” no API key needed |
| ğŸ›¡ï¸ **Rate Limiting** | Per-IP sliding-window limits for chat, weather, and jokes |
| ğŸ’¾ **SQLite Persistence** | Full message history stored in `chatbot.db` |
| âš¡ **Host Failover** | Automatic Ollama primary â†’ backup host switching |
| âš™ï¸ **Env-based Config** | All secrets and timeouts controlled via `.env` |

---

## ğŸ—‚ï¸ Project Structure

```
chatbot/
â”œâ”€â”€ app.py               # Flask app â€” routes & request handling
â”œâ”€â”€ ollama_manager.py    # Ollama LLM client + manager (streaming, failover, history)
â”œâ”€â”€ api_manager.py       # External APIs (Weather, Jokes) + rate limiting
â”œâ”€â”€ database.py          # SQLite persistence layer
â”œâ”€â”€ chatbot_model.py     # Model training script
â”œâ”€â”€ chatbot_model.h5     # Trained Keras model (binary)
â”œâ”€â”€ intents.json         # Intent definitions (patterns + responses)
â”œâ”€â”€ words.pkl            # Vocabulary pickle (generated by training)
â”œâ”€â”€ classes.pkl          # Intent classes pickle (generated by training)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .env.example         # Environment variable template
â”œâ”€â”€ .env                 # Your local config (not committed to git)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css        # UI styles
â”‚   â””â”€â”€ script.js        # Frontend logic & Ollama SSE streaming
â””â”€â”€ templates/
    â””â”€â”€ index.html       # Main chat page
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com) installed and running locally *(optional â€” NLP mode works without it)*

### 1. Clone & set up the virtual environment

```bash
git clone <your-repo-url>
cd chatbot
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment variables

```bash
cp .env.example .env
```

Edit `.env` with your preferred values. See the [Configuration](#ï¸-configuration) section for details.

### 4. (Optional) Pull Ollama models

```bash
ollama pull llama3.2
ollama pull gemma3:4b
```

> Skip this step if you only want to use the NLP intent classifier.

### 5. Run the app

```bash
python app.py
```

Open your browser at **[http://localhost:5000](http://localhost:5000)**.

---

## âš™ï¸ Configuration

All configuration is done via the `.env` file. Copy `.env.example` to get started:

| Variable | Default | Description |
|---|---|---|
| `OLLAMA_HOST` | `http://localhost:11434` | Primary Ollama server URL |
| `OLLAMA_HOST_BACKUP` | `http://127.0.0.1:11434` | Backup Ollama URL (auto-failover) |
| `OLLAMA_TIMEOUT` | `90` | Seconds to wait for a generation response |
| `OLLAMA_HEALTH_TIMEOUT` | `3` | Seconds to wait for a health-check ping |
| `FLASK_DEBUG` | `true` | Set to `false` in production |
| `FLASK_SECRET_KEY` | `change-me-in-production` | Flask session secret â€” **change this!** |
| `DB_PATH` | `chatbot.db` | Path to the SQLite database file |

---

## ğŸŒ API Endpoints

### Chat (NLP)

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/` | Serve the chat UI |
| `POST` | `/get` | NLP intent-based chatbot response |

**POST `/get`** form fields:
- `msg` â€” the user's message

---

### Ollama LLM

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/ollama/chat` | Non-streaming Ollama chat |
| `POST` | `/api/ollama/stream` | Server-Sent Events streaming chat |
| `GET` | `/api/ollama/status` | Server health + available models |
| `GET` | `/api/ollama/models` | Supported & available model list |
| `POST` | `/api/ollama/clear` | Clear in-memory + DB history for a session |

**POST `/api/ollama/chat`** JSON body:
```json
{
  "msg": "What is recursion?",
  "model": "llama3.2",
  "session_id": "optional-id"
}
```

**POST `/api/ollama/stream`** â€” same body; responds as `text/event-stream`:
```
data: {"delta": "Recursion is..."}
data: {"delta": " a function that calls itself."}
data: {"done": true, "model": "llama3.2"}
```

---

### Statistics & History

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/api/stats` | Aggregate chat statistics |
| `GET` | `/api/history?session_id=&limit=50` | Message history for a session |

---

## ğŸ§  Supported Ollama Models

| Key | Full Model Name | Description |
|---|---|---|
| `llama3.2` | `llama3.2:latest` | Fast, concise general-purpose assistant |
| `gemma3` | `gemma3:4b` | Thoughtful, well-structured responses |

---

## ğŸ›¡ï¸ Rate Limits

All limits are **per client IP**, using a sliding-window algorithm:

| Route | Limit |
|---|---|
| Chat (NLP + Ollama) | 30 messages / 60 s |
| Weather API | 10 requests / 60 s |
| Jokes API | 15 requests / 60 s |

---

## ğŸ”„ Retraining the Model

If you update `intents.json` with new patterns or responses, retrain the model:

```bash
python chatbot_model.py
```

This regenerates `chatbot_model.h5`, `words.pkl`, and `classes.pkl`.

---

## ğŸ“¦ Dependencies

| Package | Purpose |
|---|---|
| `flask` | Web framework |
| `tensorflow` | Keras NLP model inference |
| `numpy` | Numerical operations |
| `nltk` | Tokenization & lemmatization |
| `requests` | HTTP calls to Ollama & external APIs |
| `python-dotenv` | `.env` file loading |

---

## ğŸ“ License

This project is for educational purposes. 
